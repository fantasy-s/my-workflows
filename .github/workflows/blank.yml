name: Web Scraper

on:
  schedule:
    # 每小时执行一次
    - cron: '0 */6 * * *'
  workflow_dispatch: # 允许手动触发

jobs:
  scrape:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4
    
    - name: Setup Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install requests beautifulsoup4
    
    - name: Scrape website
      run: |
        python <<EOF
        import requests
        from bs4 import BeautifulSoup
        import re
        
        url = 'https://v2cross.com/1884.html'
        headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        }
        
        try:
            response = requests.get(url, headers=headers)
            response.encoding = 'utf-8'
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # 获取所有h5标签内容
            h5_contents = []
            for h5 in soup.find_all('h5'):
                text = h5.get_text().strip()
                # 去除特定文字
                text = text.replace('本次节点订阅地址：', '')
                if text:
                    h5_contents.append(text)
            
            # 将结果写入文件
            with open('scraped_content.txt', 'w', encoding='utf-8') as f:
                for content in h5_contents:
                    f.write(content + '\n')
            
            print("Scraping completed successfully!")
            
        except Exception as e:
            print(f"Error occurred: {str(e)}")
            exit(1)
        EOF
    
    - name: Save results
      uses: actions/upload-artifact@v4
      with:
        name: scraped-content
        path: scraped_content.txt
